# -*- coding: utf-8 -*-
"""k_mean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VIu002q6yclU6lxGjsFw3OQ2xdCCF6dK
"""

import random
import math
import re
import requests

def distance(tweet1, tweet2):
  splitTweet1 = set(tweet1.split())
  splitTweet2 = set(tweet2.split())
  commonWords = splitTweet1 & splitTweet2
  combinedSet = splitTweet1.union(splitTweet2)
  return 1 - (len(commonWords)/len(combinedSet))

#takes a list of tweets and finds the one with the minimum mean distance to all other tweets
def commputeCentroid(tweets):
  minDistance = float('inf')
  currentCentroid = ""
  for tweet1 in tweets:
    currentDistance = 0
    for tweet2 in tweets:
      currentDistance = currentDistance + distance(tweet1,tweet2)
    if currentDistance<minDistance:
      minDistance=currentDistance
      currentCentroid = tweet1

  return currentCentroid

def sse(clusters, centroids):
  sse = 0
  for i in range(len(centroids)):
    centroid = centroids[i]
    for tweet in clusters[i]:
      sse += distance(tweet,centroid)**2
  return sse

def K_Mean(tweets, k, maxIter=100):
  #selects k random tweets to be the starting centroid
  centroids = random.sample(tweets, k)
  clusters = []

  for i in range(maxIter):
    #gets the current clusters based on distance
    clusters = [[] for x in range(k)]
    for tweet in tweets:
      closestDistance = 2
      indexOfCentroid = -1
      for c in range(len(centroids)):
        currentDistance = distance(centroids[c], tweet)
        if(currentDistance<closestDistance):
          closestDistance = currentDistance
          indexOfCentroid = c
      clusters[indexOfCentroid].append(tweet)

    newCentroids = []
    for cluster in clusters:
      newCentroids.append(commputeCentroid(cluster))
    if all([newCentroids[i] == centroids[i] for i in range(k)]):
      break
    centroids = newCentroids

  SSE = sse(clusters, centroids)
  print("For K: "+ str(k))
  print("sse: " + str(SSE))
  i = 1
  for cluster in clusters:
    print(str(i)+": "+str(len(cluster))+" tweets")
    i += 1

def main():
  #getting the data
  URL="https://raw.githubusercontent.com/Bondzberg/data/main/tweetdata.txt"
  response = requests.get(URL)
  tweets = response.text
  tweets = tweets.split("\n")

  #preprocessing
  for i in range(len(tweets)):
    tweet = tweets[i]
    tweet = tweet[50:]
    tweet = re.sub('http[s]?://\S+', '', tweet)
    tweet = re.sub(r'@\w+','',tweet)
    tweet = tweet.replace("#","")
    tweet = tweet.replace("\r","")
    tweets[i] = tweet.lower().strip()

  Ks = [5,10, 15,20,25]
  for k in Ks:
    K_Mean(tweets, k)


main()