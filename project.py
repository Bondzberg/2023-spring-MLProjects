# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13NaqnO00xo4DuZ47ZJCAmKGyt1tZrZz_
"""

import numpy as np #used for math functions
import requests #used for getting the data off of github
import matplotlib.pyplot as plt #for plotting
from matplotlib.pyplot import figure

#meta variables
hsize = 15
iterations = 10
alpha = 0.01 #learning rate

#URL to the data
URL="https://raw.githubusercontent.com/Bondzberg/data/main/rnn_data.csv"

#variables related to weights
inputsize = 4
outputsize = 1

#initilizing weights
w_input = np.random.uniform(-1, 1, (inputsize, hsize)) #input weights
w_hidden = np.random.uniform(-1, 1 , (hsize, hsize)) #hidden layer weights
w_output = np.random.uniform(-1, 1 , (hsize,outputsize)) #output weights

#bias for hidden layer and output layer
b_hidden = np.random.uniform(-1,1,(1, hsize))
b_output = np.random.uniform(-1,1,(outputsize, 1))

#activation function used on h
def activation(x):
  return np.tanh(x)

#derivative of the activation function on h
def dactivation(x):
  return 1 - np.tanh(x)*np.tanh(x)

def forwardPass(x, h_previous):
  h = activation(np.dot(x, w_input) + np.dot(h_previous, w_hidden) + b_hidden)
  y = np.dot(h, w_output) + b_output
  return h, y

def clip_gradient(grady, gradh):
  normalized = np.sum(grady**2)
  normalized += np.sum(gradh**2)
  normalized = np.sqrt(normalized)
  if(normalized>1):
    scale = 1/normalized
    grady *= normalized
    gradh *= normalized
  return grady, gradh

def graph_Y(pred_Y, actual_Y, title):
  plt.plot(actual_Y, label='Actual')
  plt.plot(pred_Y, label='Predicted')
  plt.xlabel("days")
  plt.ylabel("price of stock")
  plt.title("Prediction - "+title)
  plt.legend()
  plt.show()

def graph_L(loss, title):
  plt.plot(loss)
  plt.xlabel("iterations")
  plt.ylabel("loss")
  plt.title("Loss - "+title)
  plt.show()

def test(train_X, train_Y):
  #cause python throws a hissy fit over "local" variables being refrenced before assignment
  global w_input, w_hidden, w_output, b_hidden, b_output

  predicted_Y = []
  loss = np.zeros(iterations)
  for i in range(iterations):
    h_previous = np.zeros(hsize)

    for x, y in zip(train_X, train_Y):
      h, pred_Y = forwardPass(x, h_previous)

      #caculate the gradients
      grady = pred_Y - y
      gradh = np.dot(grady, w_output.T) * dactivation(h)

      #update weights and biases
      w_input -= alpha * np.outer(x, gradh)
      w_hidden -= alpha * np.outer(h_previous, gradh)
      w_output -= alpha * np.outer(h, grady)

      b_hidden -= alpha * gradh
      b_output -= alpha * grady

      #get the reassign h_previous
      h_previous, _ = forwardPass(x, h_previous)

      #save training info
      loss[i] += np.sum((pred_Y-y)**2)
      if i == iterations-1:
        predicted_Y.append(pred_Y[0][0])
  return loss, predicted_Y

def preprocessing(data):
  #splits up the data,
  data = data.split("\n")
  X=[]
  Y=[]
  for i in range(1,len(data)-1):
    Xi = list(map(float,data[i].split(",")[1:4]))
    Xi.append(float(data[i].split(",")[6]))
    X.append(Xi)
    Y.append(float(data[i].split(",")[4]))

  return X, Y

def main():
  response = requests.get(URL)
  data = response.text
  X, Y = preprocessing(data)
  loss, pred_Y = test(X,Y)
  title = "Hidden size:{hsize}, learning rate: {alpha}, iterations: {iterations}".format(hsize = hsize, alpha = alpha, iterations=iterations)
  graph_Y(pred_Y, Y,title)
  graph_L(loss, title)


main()